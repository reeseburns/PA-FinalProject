{"cells":[{"cell_type":"markdown","metadata":{"id":"daf-W_Sj4FSV"},"source":["# Group 1 - Predicting Airbnb Prices in Europe\n","\n","Based on features like *room_type*, *location*, and *ratings*, can we predict Airbnb prices across major European cities?\n","\n","## Dataset Description\n","\n","***Kaggle Link:*** https://www.kaggle.com/datasets/thedevastator/airbnb-prices-in-european-cities/data?select=london_weekdays.csv\n","\n","- ***weekend:***  1 if its a weekend and 0 if it is not\n","\n","- ***host_is_superhost:***  if a host is a superhost, they meet these requirements (source: https://www.airbnb.com/help/article/829) :\n","\n","    - Hosted at least 10 reservations, or 3 reservations that total at least 100 nights\n","    - Maintained a 90% or higher response rate\n","    - Maintained a less than 1% cancellation rate, with exceptions for cancellations due to Major Disruptive Events or other valid reasons\n","    - Maintained a 4.8 or higher overall rating (A review counts towards Superhost status when either both the guest and the host have submitted a review, or the 14-day window for reviews is over, whichever comes first.)\n","\n","- ***bedrooms:***  in the case that bedrooms = 0, it might be because the listing is for a studio\n","\n","- ***multiple_room:***  1 if listing is for multiple rooms, 0 otherwise.\n","\n","- ***spare_room:***  if a listing is NOT for multiple rooms and is NOT for business purpose, then the listing is only for one room\n","\n","- ***business_purpose:***  1 if it is for business purpose (full time income generating) or not (personal use, for example renting out a spare room for a few weeks). For this classification, we assume that business listings typically feature more rooms than the threshold for multiple rooms column. This is because we noticed that there are listings that are NOT multiple rooms and NOT for business purpose.\n","\n","- ***price:*** we took the natural log of the price because we noticed that the price is very skewed."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4009,"status":"ok","timestamp":1734066014377,"user":{"displayName":"Millie Chen","userId":"17335063931910009668"},"user_tz":300},"id":"MISeA6bntbjP"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Data Preprocessing\n","import pandas as pd\n","import numpy as np\n","import zipfile\n","import os\n","\n","# Exploratory Data Analysis\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import seaborn as sns\n","\n","# Principal Component Analysis\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","from sklearn import decomposition\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"B0P4b5MFtbjQ"},"source":["## Data Preprocessing\n","\n","Let's preprocess the datasets in *Airbnb_data.zip* to understand the features and how to proceed with Exploratory Data Analysis (EDA)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":56},"id":"JoWh8emH3nNw","outputId":"45f643fe-d828-4fb7-aeb8-05f512718dd6"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# # Load Dataset - COLAB\n","# from google.colab import files\n","# uploaded = files.upload()\n","# !unzip Airbnb_data.zip\n","\n","# datasets = {}\n","# for _, _, filenames in os.walk(\"Airbnb_data\"):\n","#   for file_name in filenames:\n","#     name = file_name.split(\".\")[0]\n","#     datasets[name] = pd.read_csv(\"Airbnb_data/\" + file_name)\n","# datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bdIMlLCtbjQ"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Load Dataset - VSCode\n","path = \"Airbnb_data.zip\"\n","folder = \"Airbnb_data\"\n","\n","# extract zipfile\n","with zipfile.ZipFile(path, \"r\") as zip_ref:\n","    zip_ref.extractall(folder)\n","\n","# list extracted files\n","files = os.listdir(folder)\n","print(f\"Extracted Files: {files}\\n\")\n","\n","# load csv files\n","datasets = {}\n","\n","for file in files:\n","    if file.endswith(\".csv\"):\n","        print(f\"Loading file: {file}\")\n","        path = os.path.join(folder, file)\n","        df = pd.read_csv(path)\n","        datasets[file.rsplit(\".\", 1)[0]] = df # remove \".csv\""]},{"cell_type":"markdown","metadata":{"id":"s1VcsKrltbjR"},"source":["Now that we've loaded the airbnb datasets, we can continue with data preprocessing to prepare it for EDA.\n","\n","### i) Add \"City\" & \"Weekend\" Columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nL8A1MdptbjR"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Add \"city\" & \"weekend\" Columns\n","for name, dataset in datasets.items():\n","    city, weekend = name.split(\"_\")\n","    dataset[\"city\"] = city\n","    dataset[\"weekend\"] = weekend == \"weekends\"\n","    dataset[\"weekend\"] = dataset[\"weekend\"].astype(int)"]},{"cell_type":"markdown","metadata":{"id":"AIyo17I9tbjS"},"source":["### ii) Combine Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jE-GosJD-Zv8"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Combine Datasets\n","df = pd.concat([datasets[file_name] for file_name in datasets])\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"khnOvrvQtbjS"},"source":["***Observation:*** Based on the concatenated airbnb dataset, we see that the *unnamed* column is simply the index from the original dataset. We don't need this anymore...\n","\n","### iii) Clean Data\n","\n","First, let's standardize the column names for better readability and consistency."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6OYkf0ctbjT"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Standardize Column Names\n","name = {'realSum' :'price',\n","        'biz': 'business_purpose',\n","        'multi' : 'multiple_rooms',\n","        'dist' : 'dist_city_center',\n","        'metro_dist' : 'dist_metro_station',\n","        'lng' : 'longitude',\n","        'lat':'latitude'\n","        }\n","\n","df = df.rename(columns = name)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"VzsjfOtWtbjT"},"source":["***Observation:*** The *Unnamed* column is redundant and unecessary as it's the same as the original dataset's indices.\n","\n","So let's drop *Unnamed* from the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3QBRUpptbjT"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Drop \"Unnamed\" Column\n","df = df.drop(columns =['Unnamed: 0'])\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"bUGaVXRetbjT"},"source":["Now let's look at the descriptive statistics of our new dataset. This will give us insight into skewness and outliers in the dataset that we will need to handle."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLpV_zyltbjT"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Descriptive Statistics\n","df.describe()"]},{"cell_type":"markdown","metadata":{"id":"rreXTRwatbjU"},"source":["***Observation:*** Based on the descriptive statistics, *price* is heavily skewed.\n","\n","Let's use log to transform the *price* column so it's standardized."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"am7dUF3jtbjU"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Standardize \"price\" Column\n","df[\"price\"] = np.log(df['price'])\n","df.describe()"]},{"cell_type":"markdown","metadata":{"id":"du3bA3MitbjU"},"source":["Let's add a *spare_room* column to the dataset based on the *multiple_rooms* and *business_purpose* columns.\n","\n","This will add more insight into the types of listings users are booking."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sd0LLm1UtbjU"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Add \"spare_room\" Column\n","df['spare_room'] = ((df['multiple_rooms'] == 0) & (df['business_purpose'] == 0))\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"Ki4hE6X_tbjU"},"source":["Now that we handled inconsistent data, we can handle missing data.\n","\n","Let's see if and where our dataset has missing values so we can determine whether to drop or imputate it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqugOAmEBF6i"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Check Missing Data\n","np.sum(df.isnull(), axis = 0)"]},{"cell_type":"markdown","metadata":{"id":"xdsn7Hv_tbjU"},"source":["***Observation:*** Our dataset has no null values! So there's no need to handle missing data and we can continue with our EDA.\n","\n","## Exploratory Data Analysis (EDA)\n","\n","Next, let's take an initial look at the features in our dataset, so we can gain more insight about their roles and characteristics.\n","\n","### i) Feature Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vq1Lxnq5tbjU"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Graph Functions\n","def init_subplots(rows, cols, figsize, df):\n","    fig, axes = plt.subplots(rows, cols, figsize=figsize) # init grid side\n","    axes = axes.flatten()\n","    cmap = cm.get_cmap('inferno', len(df.columns)) # inferno colors\n","    return fig, axes, cmap\n","\n","def separate_features(df):\n","    numerical_cols = [col for col in df.columns if df[col].dtype == \"float64\"]\n","    categorical_cols = [col for col in df.columns if df[col].dtype != \"float64\"]\n","    all_columns = numerical_cols + categorical_cols\n","    return numerical_cols, categorical_cols, all_columns\n","\n","def color_palette(cmap, idx, df, col):\n","    color = cmap(idx / len(df.columns))                 # create colormap\n","    unique_vals = df[col].nunique()                     # get unique vals\n","    palette = sns.color_palette(\"inferno\", unique_vals) # map palette to unique vals\n","    return color, palette\n","\n","def show_plots(df, fig, axes, layout):\n","    for i in range(len(df.columns) - 1, len(axes)): # hide unused subplots\n","        fig.delaxes(axes[i])\n","\n","    if layout != None: fig.tight_layout(rect=layout) # setup layout\n","    else: fig.tight_layout()\n","    fig.show()\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M36X-S-NtbjU"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Initialize Plots\n","numerical_cols, categorical_cols, all_columns = separate_features(df) # separate types\n","num_numerical = len(numerical_cols)\n","num_categorical = len(categorical_cols)"]},{"cell_type":"markdown","metadata":{"id":"jkN1UxkJtbjU"},"source":["Let's plot the distributions for *numerical features* in the dataset using *histograms*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hq5IUDfWtbjU"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Histogram Distributions - Numerical Features\n","fig, axes, cmap = init_subplots(4, 3, (10,12), df) # init subplot\n","\n","for idx, col in enumerate(numerical_cols):\n","    ax = axes[idx]                               # get subplot idx\n","    color, _ = color_palette(cmap, idx, df, col) # init color\n","    sns.histplot(df, x=col, ax=ax, color=color)\n","    ax.set_title(col)\n","    ax.set_xlabel(col, fontsize=8)\n","    ax.set_ylabel(\"count\", fontsize=8)\n","\n","fig.text(0.5, 0.985, \"Numerical Features - Histogram Distributions\", fontsize=16, ha=\"center\")\n","show_plots(df, fig, axes, [0, 0, 1, 0.97]) # display plots"]},{"cell_type":"markdown","metadata":{"id":"YqUBtNfntbjV"},"source":["**Observations - Numerical**\n","\n","Based on the *histogram* distributions, we can see that...\n","\n","- ***Price:*** Heavily skewed to the right, indicating that most listing have lower prices with some high-priced outliers.\n","\n","- ***Guest Satisfaction:*** Skewed toward high satisfaction, with many ratings clustered around 100.\n","\n","- ***Distance:*** Most listings are within 5km of the city center.\n","\n","- ***Restaurants & Attractions:*** Heavily skewed to the right, indicating that most listings are near fewer restaurants/attractions, while a small number are near a high density of them.\n","\n","- ***Location:*** Based on *longitude* and *latitude*, listings are clearly concentrated in specific geographic regions as seen by the clustering.\n","\n","Now let's plot the distributions for *categorical features* in the dataset using *countplots*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8C1drlR5tbjV"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Countplot Distributions - Categorical Features\n","fig, axes, cmap = init_subplots(4, 3, (10,12), df) # init subplot\n","\n","for idx, col in enumerate(categorical_cols):\n","    ax = axes[idx]                                             # get subplot idx\n","    _, palette = color_palette(cmap, num_categorical, df, col) # init palette\n","    sns.countplot(df, x=col, ax=ax, palette=palette, hue=col, legend=False)\n","    ax.set_title(col)\n","    ax.set_xticks(ax.get_xticks())\n","    ax.set_xticklabels(ax.get_xticklabels(), rotation=60, ha=\"center\")\n","\n","for ax in axes[len(categorical_cols):]: ax.remove()\n","fig.text(0.5, 1.02, \"Categorical Features - Countplot Distributions\", fontsize=16, ha=\"center\")\n","show_plots(df, fig, axes, None) # display plots"]},{"cell_type":"markdown","metadata":{"id":"VO3gtcVBtbjV"},"source":["**Observations - Categorical**\n","\n","Based on the *countplot* distributions, we can see that...\n","\n","- ***Room Types:*** \"Entire home/apt\" is the majority of the listings, followed by \"Private room\" then \"Shared room\".\n","\n","- ***Room Features:*** A significant number of listings have private rooms where its capacity is around 2-4 people.\n","\n","- ***Superhost Status:*** Majority of hosts are not superhosts, suggesting that becoming a superhost is rare.\n","\n","- ***City Distribution:*** Majority of listsings in London, followed by Rome and Paris. There are significant variations in the number of listings across the other cities.\n","\n","- ***Weekend:*** Most listings are not flagged as \"weekend\", which might indicate mostly weekday stays or a lack of weekend-specific data.\n","\n","### ii) Correlation Analysis\n","\n","First, let's plot the correlation distributions for *numerical features* in the dataset using *scatterplots*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eReLM_gNtbjV"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Scatterplot Distributions - Numerical Features\n","fig, axes, cmap = init_subplots(4, 3, (10, 12), df) # init subplots\n","numerical_cols, _, _ = separate_features(df)        # separate types\n","\n","for idx, col in enumerate(numerical_cols):\n","    if col == \"price\":  # skip price column\n","        continue\n","\n","    else:\n","        ax = axes[idx]  # Get subplot\n","        color, _ = color_palette(cmap, idx, df, col)  # Init color\n","        sns.scatterplot(x=df[col], y=df[\"price\"], ax=ax, color=color)\n","        ax.set_title(f\"price vs {col}\")\n","        ax.set_xlabel(col, fontsize=8)\n","        ax.set_ylabel(\"price\", fontsize=8)\n","\n","fig.text(0.5, 1.02, \"Price Correlation Pairplots - Numerical Features\", fontsize=16, ha=\"center\")\n","show_plots(df, fig, axes, None)  # Display numerical pairplots"]},{"cell_type":"markdown","metadata":{"id":"q0tIvjNctbjV"},"source":["**Observations - Numerical:**\n","\n","In relation to *price*, the following numerical features are...\n","\n","- ***person_capacity:*** positively correlated\n","\n","- ***guest_satisfaction_overall:*** positively correlated\n","\n","- ***attr_index:*** positively correlated\n","\n","- ***rest_index:*** positively correlated\n","\n","- ***dist_city_center:*** negatively correlated\n","\n","- ***location:*** not significantly correlated\n","\n","Now let's plot the correlation distributions for *categorical features* in the dataset using *boxplots*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANctQqUdtbjV"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Boxplot Distributions - Categorical Features\n","fig, axes, cmap = init_subplots(4, 3, (10, 12), df) # init subplots\n","_, categorical_cols, _ = separate_features(df)      # separate types\n","\n","for idx, col in enumerate(categorical_cols):\n","    ax = axes[idx]  # Get subplot\n","    _, palette = color_palette(cmap, idx, df, col)  # init palette\n","    sns.boxplot(x=df[col], y=df[\"price\"], ax=ax, palette=palette)\n","    ax.set_title(f\"price vs {col}\")\n","    ax.set_xticklabels(ax.get_xticklabels(), rotation=60, ha=\"center\")\n","    ax.set_xlabel(col, fontsize=8)\n","    ax.set_ylabel(\"price\", fontsize=8)\n","\n","for ax in axes[len(categorical_cols):]: ax.remove()\n","fig.text(0.5, 1.02, \"Price Correlation Boxplots - Categorical Features\", fontsize=16, ha=\"center\")\n","show_plots(df, fig, axes, None)  # display plots"]},{"cell_type":"markdown","metadata":{"id":"T0Li2RXEtbjV"},"source":["**Observations - Categorical:**\n","\n","In relation to *price*, the following categorical features are...\n","\n","- ***room_type:*** positively correlated\n","\n","- ***host_is_superhost:*** positively correlated but weak\n","\n","- ***multiple_rooms:*** positively correlated\n","\n","- ***business_purpose:*** positively correlated\n","\n","- ***city:*** not linearly correlated\n","\n","- ***weekend:*** not correlated\n","\n","## Principal Component Analysis (PCA)\n","\n","Let's use *PCA Clustering* to gain more insight about the structure, relationships, and patterns in our dataset.\n","\n","### i) Train, Test Split\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVgoBgijYpcY"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Train, Test, Split\n","X = df.drop(columns = ['price']) # drop \"price\" col\n","y = df['price']\n","\n","# split into train, test, validation sets\n","X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.15, random_state = 1234, shuffle = True)\n","X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.17647, random_state = 1234, shuffle = True)"]},{"cell_type":"markdown","metadata":{"id":"-rrDRBr9tbjV"},"source":["### ii) Standardize Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Dn2o9UztbjV"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Get Data\n","X_df = X_train\n","X_df = X_df.drop(columns = ['room_type']) # drop bc already encoded\n","X_df = pd.get_dummies(X_df)\n","X_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6P28kK3tbjW"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Standardize Data\n","scaler = preprocessing.StandardScaler()\n","scaled_X_train_df = scaler.fit_transform(X_df)\n","\n","# standardize X_test and X_valid\n","scaled_X_valid_df = X_valid.drop(columns = ['room_type'])\n","scaled_X_valid_df = pd.get_dummies(scaled_X_valid_df)\n","scaled_X_valid_df = scaler.transform(scaled_X_valid_df)\n","\n","scaled_X_test_df = X_test.drop(columns = ['room_type'])\n","scaled_X_test_df = pd.get_dummies(scaled_X_test_df)\n","scaled_X_test_df = scaler.transform(scaled_X_test_df)"]},{"cell_type":"markdown","metadata":{"id":"-szAQUrItbjW"},"source":["### iii) Conduct PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRtAn7IKTKXh"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Compute Explained Variance\n","pca = decomposition.PCA(random_state = 1234).fit(scaled_X_train_df)                  # perform PCA\n","explained_var = round(sum(pca.explained_variance_ratio_[:13]), 4) # compute explained variance\n","print(f\"Explained Variance = {explained_var}\")"]},{"cell_type":"markdown","metadata":{"id":"Sjh44VDStbjb"},"source":["***Observation:*** Based on the explained variance computer above, the first 13 principal components explain around 79% of the variability in the data. This suggests that, after PCA, the reduced dimensions are still highly representative of the original data.\n","\n","Let's transform the *training* data.\n","\n","Note: We only want to keep the first 13 principal components because our model is able to explain around 79% of the variance in the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0vjBtWuTKSD"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Transformation - Training Data\n","scaled_X_train_df = pca.transform(scaled_X_train_df)\n","scaled_X_train_df = pd.DataFrame(scaled_X_train_df)\n","scaled_X_train_df.columns = ['PC_' + str(i) for i in range(1, 30)]\n","scaled_X_train_df = scaled_X_train_df.iloc[:,0:13]\n","scaled_X_train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"WQsjsTwUtbjb"},"source":["Let's take a look at the *eigenvectors* in the dataset.\n","\n","They will help us determine how much each original feature contributes (weighs) to a principal component."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"2fQQ_VGSWZIL"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Get Eigenvectors\n","eigenvectors = pd.DataFrame(pca.components_, columns =['PC_' + str(i) for i in range(1, 30)])\n","eigenvectors = eigenvectors.iloc[:, 0:13]\n","eigenvectors.head()"]},{"cell_type":"markdown","metadata":{"id":"dXpbYsOTtbjb"},"source":["Now let's visualize the eigenvectors for each principal component. More specifically, let's look at how much of each feature contributed to the corresponding principal component."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhYEDcUStbjb"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Eigenvector Barplots\n","fig, axes, cmap = init_subplots(8, 3, (10,20), df) # init plots\n","\n","for idx, PC in enumerate(eigenvectors.columns):\n","    ax = axes[idx] # get subplot idx\n","    palette = sns.color_palette(\"inferno\", n_colors=len(eigenvectors.index))\n","\n","    sns.barplot(x=eigenvectors.index, y=eigenvectors[PC], ax=ax, palette=palette)\n","    ax.set_title(PC)\n","    ax.set_xlabel(\"OG Features\", fontsize=8)\n","    ax.set_ylabel(\"Contribution\", fontsize=8)\n","\n","    # setup x-ticks parameters\n","    xticks = ax.get_xticks()\n","    ax.set_xticks(xticks[::5]) # skip every 5 ticks\n","    ax.tick_params(axis='x')\n","\n","fig.suptitle(\"Eigenvector Contributions\", fontsize=14, y=1)\n","show_plots(eigenvectors, fig, axes, None) # display plots"]},{"cell_type":"markdown","metadata":{"id":"gM2dmhjKtbjb"},"source":["***Observations:*** Based on these principal component barplots, many of the components have a single feature that has a significant, positive contribution. This suggests that those features contribute the most in terms of explaining the variance captured in that given principal component.\n","\n","Now let's transform the *validation* and *test* data too so we can visualize the PCA results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVWhuLK6gcHK"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Transformation - Valid Data\n","scaled_X_valid_df = pca.transform(scaled_X_valid_df)\n","scaled_X_valid_df = pd.DataFrame(scaled_X_valid_df)\n","scaled_X_valid_df.columns = ['PC_' + str(i) for i in range(1, 30)]\n","scaled_X_valid_df = scaled_X_valid_df.iloc[:,0:13]\n","scaled_X_valid_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DX5SIXAtbjb"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Transformation - Test Data\n","scaled_X_test_df = pca.transform(scaled_X_test_df)\n","scaled_X_test_df = pd.DataFrame(scaled_X_test_df)\n","scaled_X_test_df.columns = ['PC_' + str(i) for i in range(1, 30)]\n","scaled_X_test_df = scaled_X_test_df.iloc[:,0:13]\n","scaled_X_test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxPcfhONtbjc"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# PCA Scatterplot\n","plt.figure(figsize=(8, 6))\n","plt.scatter(scaled_X_valid_df[\"PC_1\"], scaled_X_valid_df[\"PC_2\"], alpha=0.6, label=\"Validation Data\", color=\"#84206b\")\n","plt.scatter(scaled_X_test_df[\"PC_1\"], scaled_X_test_df[\"PC_2\"], alpha=0.6, label=\"Test Data\", color=\"#e55c30\")\n","plt.title(\"PCA Results: Validation and Test Data\")\n","plt.legend()\n","plt.xlabel(\"PC_1\")\n","plt.ylabel(\"PC_2\")\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"W9vqK3vAtbjc"},"source":["**Observations**\n","\n","There's a significant overlap between *validation* and *test* datasets in the PCA space. This suggests...\n","\n","- ***Consistency:*** The feature representation across these datasets is consistent\n","\n","- ***Simplicity:*** Without distinct clustering and separation, the dataset doesn't show strong non-linear patterns\n","\n","- ***Generalizability:*** Components can effectively represent unseen data (validation and test sets)\n","\n","Based on the *PCA*, the identified principal components capture the variance in the dataset while reducing multicolinearity among features."]},{"cell_type":"markdown","metadata":{"id":"Aul0ds5cI-Lr"},"source":["## Method I - Multi-Linear Regression (MLR)\n","\n","For our first method, let's use *Multi-Linear Regression (MLR)* to build a model for predicting Airbnb prices while mitigating overfitting.\n","\n","This model will help us to uncover the relationship between a *dependent* variable (price) and multiple *independent variables*.\n","\n","### i) Initialize Training & Testing Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dSFMN7pssx1Z"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-E7xWM0Qsx1Z"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Initialize Variables (X_train)\n","X_train = pd.get_dummies(X_train, columns=[\"room_type\", \"city\"], drop_first=True)\n","X_train_np = X_train.values.astype(float)\n","X_train_np = np.hstack((X_train_np, np.ones((X_train_np.shape[0], 1)))) # add bias\n","y_train_np = y_train.values.astype(float)\n","\n","# Initialize Variables (X_test)\n","X_test = pd.get_dummies(X_test, columns=[\"room_type\", \"city\"], drop_first=True)\n","X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n","X_test_np = X_test.values.astype(float)\n","X_test_np = np.hstack((X_test_np, np.ones((X_test_np.shape[0], 1)))) # add bias"]},{"cell_type":"markdown","metadata":{"id":"VKYtsUqdtbjc"},"source":["### ii) Conduct Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OMYU0LBsx1Z"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Multi-Linear Regression\n","MLR_model = LinearRegression()\n","MLR_model.fit(X_train_np, y_train_np) # fit"]},{"cell_type":"markdown","metadata":{"id":"mC4vjYkJtbjc"},"source":["### iii) Evaluate Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t-2iZhnMsx1Z"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Functions for Evaluation Metrics\n","def adjusted_r2(y_test, X_train_np, r2):\n","    n = len(y_test)          # num observations\n","    k = X_train_np.shape[1]  # num predictors\n","    MLR_adjusted_r2 = 1 - ((1 - r2) * (n - 1)/(n - k - 1)) # adjusted R^2\n","    return MLR_adjusted_r2\n","\n","def eval_metrics(mse, r2, adjusted_r2):\n","    print(f\"Test MSE:    {mse:.3f}\")\n","    print(f\"Test R²:     {r2:.4f}\")\n","    print(f\"Adjusted R²: {adjusted_r2:.4f}\")\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NzQqELpitbjc"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Evaluate MLR Model\n","y_pred = MLR_model.predict(X_test_np)                     # prediction\n","MLR_mse = mean_squared_error(y_test.values, y_pred)       # mean-squared-error\n","MLR_r2 = r2_score(y_test.values, y_pred)                  # unadjusted R^2\n","MLR_adjusted_r2 = adjusted_r2(y_test, X_train_np, MLR_r2) # adjusted R^2\n","eval_metrics(MLR_mse, MLR_r2, MLR_adjusted_r2)            # print metrics"]},{"cell_type":"markdown","metadata":{"id":"OpdVTGcCtbjc"},"source":["***Observations:*** These evaluation metric scores suggest that our model performs *reasonably well*. Since ~34% of the variability is not explained, this is likely due to missing features, randomness, and noise in our data.\n","\n","## Analysis\n","\n","### i) Numerical Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JgLonyhPtbjd"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Compare Actual vs Predicted Prices\n","y_scaler = preprocessing.StandardScaler()\n","y_scaler.fit(y_train.values.reshape(-1, 1)) # fit on y_train data\n","\n","# map city columns\n","city_columns = [col for col in X_test.columns if col.startswith(\"city_\")] # find city cols\n","city_map = {col: col.split(\"_\")[1] for col in city_columns}               # map col to city names\n","X_test[\"city\"] = X_test[city_columns].idxmax(axis=1).map(city_map)        # find city name for each row\n","\n","# create comparison_df\n","comparison_df = pd.DataFrame({\n","    \"City\" : X_test[\"city\"],        # get city\n","    \"Actual Price\" : y_test.values, # actual prices\n","    \"Predicted Price\" : y_pred      # predicted prices\n","})\n","\n","# inverse transformations on prices\n","comparison_df[\"Actual Price\"] = y_scaler.inverse_transform(comparison_df[[\"Actual Price\"]].values)\n","comparison_df[\"Actual Price\"] = np.exp(comparison_df[\"Actual Price\"])\n","\n","comparison_df[\"Predicted Price\"] = y_scaler.inverse_transform(comparison_df[[\"Predicted Price\"]].values)\n","comparison_df[\"Predicted Price\"] = np.exp(comparison_df[\"Predicted Price\"])\n","\n","# group by city and compute avg prices\n","comparison_grouped = comparison_df.groupby(\"City\").mean()\n","comparison_grouped = comparison_grouped.applymap(lambda x: f\"{x:,.2f}\" if isinstance(x, (int, float)) else x)\n","comparison_grouped.reset_index(inplace=True)\n","\n","print(\"  Actual vs Predicted Airbnb Prices ($)\")\n","comparison_sorted = comparison_grouped.sort_values(by=\"Actual Price\", ascending=True) # sort prices in ascending order\n","comparison_sorted"]},{"cell_type":"markdown","metadata":{"id":"GdnzmAWgtbjd"},"source":["In the table above are the average *average* and *predicted* prices of Airbnb listings per European city.\n","\n","- ***Pros:*** The *predicted* prices align closely with the *actual* prices across all cities, indicating that our model performs well overall.\n","\n","- ***Cons:*** There are slight variations, like in *London* and *Paris*, where the *predicted* values slightly underestimate the *actual* prices.\n","\n","### ii) Graphical Comparison\n","\n","Second, let's create several plots to visually analyze how our *MLR* model performed in predicting Airbnb prices.\n","\n","Let's look at a *scatterplot*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UZTQwmoI-Ls"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Function - Plot Actual vs Predicted Prices\n","def comparison_scatterplot(y_test, y_pred, filename=None):\n","    min_price = min(y_test.min(), y_pred.min()) # compute min\n","    max_price = max(y_test.max(), y_pred.max()) # compute max\n","\n","    fig, ax = plt.subplots(figsize=(8, 6))\n","    ax.scatter(y_test, y_pred, alpha=0.7, color=\"#84206b\", label=\"City Data\")\n","    ax.plot([min_price, max_price], [min_price, max_price], \"r--\", color=\"#e55c30\", label=\"Ideal Line\")\n","    ax.set_title(\"Actual vs Predicted Airbnb Prices\")\n","    ax.set_xlabel(\"Actual\")\n","    ax.set_ylabel(\"Predicted\")\n","    ax.legend()\n","\n","    if filename: fig.savefig(filename) # save figure for later\n","\n","    return fig.show(), fig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tiV9PSJmtbjd"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Scatterplot\n","MLR_scatterplot = comparison_scatterplot(y_test, y_pred, filename=\"MLR_scatterplot.png\")\n","MLR_scatterplot"]},{"cell_type":"markdown","metadata":{"id":"-uSiGq0etbjd"},"source":["Based on our *scatterplot* of actual vs predicted prices, our model...\n","\n","- ***Performance:*** performs reasonably well given many predictions align closely with the *ideal line*\n","\n","- ***Errors:*** contains some outliers that indicate areas where our model's predictions deviate more significantly from *actual* prices\n","\n","Now let's look at a *barplot*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kq8q7qE8tbjd"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Barcharts\n","comparison_sorted[\"Actual Price\"] = comparison_sorted[\"Actual Price\"].replace(',', '', regex=True).astype(float)\n","comparison_sorted[\"Predicted Price\"] = comparison_sorted[\"Predicted Price\"].replace(',', '', regex=True).astype(float)\n","\n","# extract sorted values\n","cities = comparison_sorted[\"City\"].values                      # city names\n","actual_prices = comparison_sorted[\"Actual Price\"].values       # actual prices\n","predicted_prices = comparison_sorted[\"Predicted Price\"].values # predicted prices\n","position = np.arange(len(cities))                              # x-axis position\n","\n","# combine actual/predicted prices\n","all_prices = np.concatenate((actual_prices, predicted_prices))\n","min_price = np.floor(all_prices.min() / 500) * 500  # round to neareast 500\n","max_price = np.ceil(all_prices.max() / 500) * 500\n","\n","# plot\n","plt.figure(figsize=(6, 4))\n","plt.bar(position - 0.2, actual_prices, width=0.4, label=\"Actual Price\", color=\"#84206b\")\n","plt.bar(position + 0.2, predicted_prices, width=0.4, label=\"Predicted Price\", color=\"#e55c30\")\n","plt.title(\"Actual vs Predicted Airbnb Prices by City\")\n","plt.xlabel(\"City\")\n","plt.ylabel(\"Price ($)\")\n","plt.xticks(position, cities, rotation=45)     # city names\n","plt.ylim(min_price, max_price)                # ensure y-axis range matches data\n","plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7) # grid for y-axis only\n","plt.tight_layout()\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"C3a_DmsYtbjd"},"source":["Based on our *barplot* of actual vs predicted prices by city, our model...\n","\n","- ***Predictions:*** The *actual* Airbnb prices tend to be higher than our *predicted* prices for all cities.\n","\n","- ***Highest Prices:*** Paris has the highest Airbnb prices, followed by London and Barcelona.\n","\n","- ***Lowest Prices:*** Budapest has the lowest Airbnb prices, followed by Rome then Lisbon.\n","\n","### Conclusion\n","\n","We conclude that our *Multi-Linear Regression* model...\n","\n","- Demonstrates a reasonable ability to predict Airbnb prices across various European cities.\n","\n","- Contains certain city-specific factors may require additional consideration to enhance prediction accuracy.\n","\n","- Provides insights that can help property owners and managers develop data-driven pricing strategies that reflect both competitive standards and localized dynamics.\n","\n","***Suggestions:*** Because we can improve our model's performance, let's try more complex models like *Gradient Boosting* and *Deep Neural Networks* in order to more accurately predict Airbnb prices.These models will help us to capture complex, non-linear relationships between multiple features and *price*."]},{"cell_type":"markdown","metadata":{"id":"VpXfy6FGI-Ls"},"source":["## Method II - Gradient Boosting\n","\n","For our second method, let's use *Gradient Boosting* which will help us to...\n","\n","- Handle outliers\n","- Get feature interactions\n","- Compute feature importance analysis\n","\n","### i) Prepare Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ugSWl6RI-Ls"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Import Packages\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSvq39bYyECH"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# One-Hot Encoding\n","df[\"room_shared\"] = df[\"room_shared\"].apply(lambda x: 1 if x else 0)\n","df[\"room_private\"] = df[\"room_private\"].apply(lambda x: 1 if x else 0)\n","df[\"host_is_superhost\"] = df[\"host_is_superhost\"].apply(lambda x: 1 if x else 0)\n","df[\"spare_room\"] = df[\"spare_room\"].apply(lambda x: 1 if x else 0)\n","\n","df = pd.get_dummies(df, columns=[\"room_type\"], drop_first=False)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"thkD9TnDI-Lt"},"source":["### ii) Conduct Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AniihzM6I-Lt"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Run Model\n","X = df.drop(columns=[\"price\", \"city\"]) # drop target/non-predictive cols\n","y = df[\"price\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)               # train\n","GB_X_train = X_train # need for later\n","GB_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42) # build model\n","GB_model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"QzxOSAoMI-Lt"},"source":["### iii) Evaluate Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKoQPG9PI-Lt"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Evaluate GB Model\n","y_pred = GB_model.predict(X_test)                       # make predictions\n","GB_mse = mean_squared_error(y_test, y_pred)             # compute MSE\n","GB_r2 = r2_score(y_test, y_pred)                        # compute R^2\n","GB_adjusted_r2 = adjusted_r2(y_test, X_train_np, GB_r2) # adjusted R^2\n","eval_metrics(GB_mse, GB_r2, GB_adjusted_r2)             # print metrics"]},{"cell_type":"markdown","metadata":{"id":"zXjakKY0I-Lt"},"source":["***Observations:*** These evaluation metric scores suggest that our model performs *well*! Since ~24% of the variability is not explained, our model's performance has improved from the *MLR* model.\n","\n","## Analysis\n","\n","Let's graphically visualize our results with a comparative *scatterplot*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOSim-u-I-Lt"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Scatterplot\n","GB_scatterplot = comparison_scatterplot(y_test, y_pred, filename=\"GB_scatterplot.png\")\n","GB_scatterplot"]},{"cell_type":"markdown","metadata":{"id":"iVbt0A0qI-Lt"},"source":["***Observations:*** Compared to our *MLR* model, the *Gradient Booster* model shows a tighter clustering of points along the ideal line, which indicates a better alignment between *predicted* and *actual* prices.\n","\n","### Conclusion\n","\n","Unlike our *Multi-Linear Regression* model, this model...\n","\n","- ***Handles Non-Linearity:*** better captures complex, non-linear relationships between features *(room_type, dist_city)* and *price*\n","\n","- ***Feature Interactions:*** identifies and models interactions between features like *[room_type, number_of_guests] or [city, availability]*, which leads to improved predictions\n","\n","- ***Minimized Errors:*** decreased *MSE* in most cities, especially for cities with more variability *(Vienna and Budapest)*"]},{"cell_type":"markdown","metadata":{"id":"r_l6myC3I-Lt"},"source":["## Method III - Deep Neural Networks\n","\n","For our third method, let's use *Deep Neural Networks* which will help us to...\n","\n","- Improve predictive accuracy on large datasets\n","- Explore advanced feature representations\n","\n","### i) Prepare Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meKuI-q8I-Lt"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.metrics import mean_absolute_error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0mksOqdmI-Lt"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["\n","# Seperate target and feature variables\n","X = df.drop('price', axis=1)\n","y = df['price']\n","\n","# Encode categorical columns\n","categorical_columns = X.select_dtypes(include=['object']).columns\n","numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n","X_encoded = pd.get_dummies(X, columns=categorical_columns)\n","\n","# Scale numerical columns\n","scaler = StandardScaler()\n","X_scaled = X_encoded.copy()\n","X_scaled[numerical_columns] = scaler.fit_transform(X_scaled[numerical_columns])\n","\n","# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.17647, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"p-x-jUfaI-Lt"},"source":["### ii) Conduct Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOdxxuoTI-Lt"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Define the model with tuned hyperparameters\n","def build_model(input_shape):\n","    model = keras.Sequential([\n","        layers.Dense(288, activation='relu', input_shape=[input_shape]),\n","        layers.Dense(320, activation='relu'),\n","        layers.Dense(32, activation='relu'),\n","        layers.Dense(1)\n","    ])\n","    optimizer = keras.optimizers.Adam(learning_rate=0.0002)\n","    model.compile(optimizer=optimizer, loss='mean_squared_error')\n","    return model\n","\n","# Train\n","input_shape = X_train.shape[1]\n","model = build_model(input_shape)\n","history = model.fit(\n","    X_train, y_train,\n","    validation_split=0.17647,\n","    epochs=100,\n","    batch_size=32,\n","    callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n","    verbose=1\n",")"]},{"cell_type":"markdown","metadata":{"id":"lP8-RxLkI-Lt"},"source":["### iii) Evaluate Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjQsso1WI-Lt"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Evaluate the model\n","y_pred = model.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","print(f\"Root Mean Squared Error: {rmse}\")\n","\n","# Plot training history\n","plt.figure(figsize=(10, 6))\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Mean Squared Error')\n","plt.title('Model Training History')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"K_tmsHBBI-Lu"},"source":["**Observations**\n","\n","- ***Initial Convergence:*** training loss has a steep decline and stabilizes quickly, indicating that the model learns effectively\n","- ***After Convergence:*** both losses reach a pleteau with minimal fluctuations, which suggests that the model achieved a stable state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q84zNUFWI-Lu"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Compute Evaluation Metrics\n","DNN_mae = mean_absolute_error(y_test, y_pred)             # mean-absolute error\n","DNN_mse = mean_squared_error(y_test, y_pred)              # mean-squared error\n","DNN_rmse = np.sqrt(DNN_mse)                               # root-mean-squared error\n","DNN_r2 = r2_score(y_test, y_pred)                         # R^2\n","DNN_adjusted_r2 = adjusted_r2(y_test, X_train_np, DNN_r2) # adjusted R^2\n","eval_metrics(DNN_mse, DNN_r2, DNN_adjusted_r2)            # print metrics"]},{"cell_type":"markdown","metadata":{"id":"gZ4ZjJkPI-Lu"},"source":["***Observations:*** Based on these evaluation metrics scores, our *Deep Neural Networks* model performs well and indicates a good fit.\n","\n","### Analysis\n","\n","Let's graphically visualize our results with a *scatterplot*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uDk04m_I-Lu"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Scatterplot\n","DNN_scatterplot = comparison_scatterplot(y_test, y_pred, filename=\"DNN_scatterplot.png\")\n","DNN_scatterplot"]},{"cell_type":"markdown","metadata":{"id":"RqXwg3fCI-Lu"},"source":["***Observations:*** Compared to our *Gradient Booster* model, the *Deep Neural Network* model shows more dispersed clustering of points along the ideal line, which indicates a slightly worse alignment between *predicted* and *actual* prices.\n","\n","Now let's plot our residuals from this model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KR2OG_zFI-Lu"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Residuals Scatterplot\n","residuals = y_test - y_pred.flatten() # flatten\n","\n","plt.figure(figsize=(8, 6))\n","plt.scatter(y_pred, residuals, alpha=0.5, color=\"#84206b\")\n","plt.axhline(y=0, linestyle='--', color=\"#e55c30\")\n","plt.title('Residuals Plot')\n","plt.xlabel('Predicted Price')\n","plt.ylabel('Residuals')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"T-8n9kdNI-Lu"},"source":["***Observations:*** The residuals are distributed around the zero line, indicating that the model has minimal bias in its predictions. However, there are a few significant outliers where the residuals are far from zero.\n","\n","So let's try to optimize our *Deep Neural Networks* model with Tensorflow's *keras_tuner* package.\n","\n","### i) Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nk7fmGgPI-Lu"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Run Optimal DNN Model\n","import keras_tuner as kt\n","\n","def build_model(hp):\n","    model = keras.Sequential()\n","    model.add(keras.layers.Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=32), activation='relu', input_shape=(X_train.shape[1],)))\n","\n","    for i in range(hp.Int('num_layers', 1, 4)):\n","        model.add(keras.layers.Dense(units=hp.Int(f'units_{i+2}', min_value=32, max_value=512, step=32), activation='relu'))\n","\n","    model.add(keras.layers.Dense(1))\n","    model.compile(optimizer=keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')), loss='mean_squared_error')\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"ipgE8xvHI-Lu"},"source":["### ii) Tune Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBiy50D4I-Lu"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Tune Hyperparameters\n","tuner = kt.Hyperband(\n","    build_model,\n","    objective='val_loss',\n","    max_epochs=100,\n","    factor=3,\n","    directory='my_dir',\n","    project_name='airbnb_price_prediction')\n","\n","stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n","tuner.search(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[stop_early])\n","best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","print(\"The hyperparameter search is complete.\\n\")\n","print(f\"Optimal # of Layers:   {best_hps.get('num_layers')}\")\n","print(f\"Optimal Learning Rate: {best_hps.get('learning_rate'):.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"n6C4gNJ1I-Lu"},"source":["### iii) Evaluate Optimal Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uz7p67mvI-Lu"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Evaluate Optimal Model\n","model = tuner.hypermodel.build(best_hps)\n","history = model.fit(X_train, y_train, epochs=100, validation_split=0.2) # fit model\n","\n","test_loss = model.evaluate(X_test, y_test)\n","print(f\"Test Loss: {test_loss}\")"]},{"cell_type":"markdown","metadata":{"id":"edEf9pQjI-Lu"},"source":["### Conclusion\n","\n","We conclude that our *Neural Network* model...\n","\n","- ***Performance:*** demonstrates a reasonable ability to *predict* Airbnb prices based on the accuracy and R^2\n","\n","- ***Suggestions:***\n","    - Further tune parameters for optimal prediction\n","    - Potentially overfits the data due to the validation loss converging with training loss and then begins to increase. Could be a result of containing such a large sum of data"]},{"cell_type":"markdown","metadata":{"id":"zg8Jn1KBI-Lu"},"source":["## Executive Summary\n","\n","### i) Compare Model Performances\n","\n","Let's compare our *Multi-Linear Regression*, *Gradient Booster*, and *Deep Neural Networks* models to identify which performed the best at predicting Airbnb prices."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-tb2XNnI-Lu"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Compare Results\n","saved_plots = [\n","    (\"Multi-Linear Regression\", \"MLR_scatterplot.png\"),\n","    (\"Gradient Boosting\", \"GB_scatterplot.png\"),\n","    (\"Deep Neural Network\", \"DNN_scatterplot.png\")\n","]\n","for title, filename in saved_plots:\n","    img = plt.imread(filename)\n","    plt.figure(figsize=(6, 4))\n","    plt.imshow(img, aspect=\"auto\") # img fill plot space\n","    plt.axis(\"off\")\n","    plt.title(title, pad=10)       # title padding\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zggMNujJI-Lu"},"source":["Our *Gradient Boosting* model has the most dense clustering around the ideal line, indicating that it performed the best at predicting Airbnb prices out of the three models.\n","\n","Let's look at our *evaluation metrics* MSE and R² now."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2oA-6JTbI-Lu"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Compare Evaluation Metrics\n","eval_metrics = {\n","    \"Multi-Linear Regression\": [round(MLR_mse, 3), round(MLR_r2, 4), round(MLR_adjusted_r2, 4)],\n","    \"Gradient Boosting\": [round(GB_mse, 3), round(GB_r2, 4),round(GB_adjusted_r2, 4)],\n","    \"Deep Neural Networks\": [round(DNN_mse, 3), round(DNN_r2, 4),round(DNN_adjusted_r2, 4)]\n","}\n","eval_metrics_df = pd.DataFrame.from_dict(eval_metrics, orient=\"index\", columns=[\"MSE\", \"R²\", \"Adjusted R²\"])\n","eval_metrics_df = eval_metrics_df.sort_values(by=\"MSE\", ascending=True) # sort by MSE\n","eval_metrics_df"]},{"cell_type":"markdown","metadata":{"id":"iebAMLH9I-Lv"},"source":["The *Gradient Boosting* has the smallest *MSE* and largest *R²*, which indicates that this model did indeed perform the best at predicting Airbnb prices.\n","\n","### ii) Graphical Analysis\n","\n","Let's look more closely at the *predicted* versus *actual* price comparisons for each European city using *Gradient Boosting*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6rMaMukI-Lv"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Gradient Boosting - Actual vs Predicted by City\n","cities = df[\"city\"].unique() # get unique cities\n","city_models = {}\n","city_results = {}\n","plot_data = {}    # store data for plotting\n","\n","# Run GB Model For Each City\n","for city in df[\"city\"].unique():\n","    city_df = df[df[\"city\"] == city]\n","    X_city = city_df.drop([\"price\", \"city\"], axis=1)\n","    y_city = city_df[\"price\"]\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X_city, y_city, test_size=0.2, random_state=42)  # split\n","    model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42) # model\n","    model.fit(X_train, y_train)\n","\n","    y_pred = model.predict(X_test) # make predictions\n","    mse = np.mean((y_pred - y_test.values) ** 2) # compute MSE\n","    r2 = r2_score(y_test, y_pred)                # compute R^2\n","\n","    # store results\n","    city_models[city] = model\n","    city_results[city] = {\"MSE\": round(mse, 3), \"R^2\": round(r2, 3)}\n","    plot_data[city] = {\"y_test\": y_test.values, \"y_pred\": y_pred}  # save data for plotting\n","\n","# Evaluate Each City\n","city_results_df = pd.DataFrame(city_results).T   # cities as rows\n","city_results_df.reset_index(inplace=True)        # reset index\n","city_results_df.columns = [\"City\", \"MSE\", \"R^2\"] # rename cols\n","city_results_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ydQgn0HI-Lv"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Scatterplots - Actual vs Predicted by City\n","fig, axes = plt.subplots(5, 2, figsize=(9, 18)) # init subplots\n","axes = axes.flatten()                           # flatten axes array\n","\n","for idx, (city, data) in enumerate(plot_data.items()):\n","    ax = axes[idx] # get subplot idx\n","    y_test = data[\"y_test\"]\n","    y_pred = data[\"y_pred\"]\n","\n","    ax.scatter(y_test, y_pred, alpha=0.7, color=\"#84206b\", label=\"Data Points\")\n","    ax.plot(y_test, y_test, \"r--\", color=\"#e55c30\", label=\"Ideal Line\")\n","    ax.set_title(f\"Prices in {city.title()}\")\n","    ax.set_xlabel(\"Actual\")\n","    ax.set_ylabel(\"Predicted\")\n","    ax.legend()\n","\n","fig.suptitle(\"Actual vs. Predicted Prices by City\", fontsize=16, y=1)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zq5geglLI-Lv"},"source":["Based on our *scatterplot* of actual vs predicted prices for each city, our model...\n","\n","- ***Performance:*** performs well across most cities while many cities *(Amsterdam, Berlin, and Rome)* demonstrate a strong fit\n","\n","- ***Errors:*** show minor deviations for certain cities *(Budapest and Vienna)* based on the greater dispersion in their scatterplots\n","\n","Now let's use *Gradient Boosting* to plot a *feature importance* barchart."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yWjIrwxI-Lv"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Barchart - Feature Importance\n","importance_vals = GB_model.feature_importances_     # GradientBooster model\n","features = GB_X_train.columns                       # feature names\n","norm = plt.Normalize(importance_vals.min(), importance_vals.max()) # normalize scale\n","temp = np.argsort(importance_vals)[::-1]            # sort by importance\n","indices = [i for i in temp if features[i]]          # get idx\n","colors = cm.inferno(norm(importance_vals[indices])) # color cmap\n","\n","# plot with inferno cmap\n","plt.figure(figsize=(8, 6))\n","plt.bar(range(len(indices)), importance_vals[indices], align=\"center\", color=colors)\n","plt.xticks(range(len(indices)), [features[i] for i in indices], rotation=50, ha=\"right\") # rotate labels\n","plt.title(\"Feature Importance\")\n","plt.xlabel(\"Feature\")\n","plt.ylabel(\"Importance\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTZdLek5I-Lv"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# Most Influential Features\n","top_5 = [features[i] for i in indices[:5]] # extract\n","last_5 = [features[i] for i in indices[-5:]]\n","\n","top_5_importance = [round(importance_vals[i],2) for i in indices[:5]] # importance vals\n","last_5_importance = [round(importance_vals[i],4) for i in indices[-5:]]\n","\n","top_features = {\"Feature\": top_5, \"Importance\": top_5_importance} # dict\n","last_features = {\"Feature\": last_5, \"Importance\": last_5_importance}\n","\n","top_features_df = pd.DataFrame(top_features) # convert to DF\n","last_features_df = pd.DataFrame(last_features)\n","\n","print(f\"Most Influential Features:\\n{top_features_df}\\n\")\n","print(f\"Least Influential Features:\\n{last_features_df}\")"]},{"cell_type":"markdown","metadata":{"id":"ptphyS1yI-Lv"},"source":["### iii) Insights\n","\n","Based on our optimal *Gradient Boosting* results, we gained practical business insights that *hosts* and *property owners* on Airbnb can benefit from.\n","\n","- ***location:*** Since more centrally located listings tend to have higher prices, hosts can emphasize location or key areas in their listing descriptions.\n","\n","- ***attr_index:*** Since the availability of nearby attractions heavily impacts pricing, hosts can highlight these attractions in their listings.\n","\n","- ***person_capacity:*** Since listings that accommodate more people are priced higher, hosts can consider adding more beds to increase capacity and justify higher pricing.\n","\n","- ***entire_home/apt:*** Since listings that offer the entire space tend to have higher prices, hosts can adjust listings to be tailored towards guests looking to book entire spaces.\n","\n","- ***bedrooms:*** Since the number of bedrooms increases prices, hosts can emphasize bedroom features or add more beds."]},{"cell_type":"markdown","metadata":{"id":"lY_WYO1mI-Lv"},"source":["## Conclusion\n","\n","In conclusion, our analysis demonstrates that *Gradient Boosting* is the most effective model for predicting Airbnb prices across major European cities. *Gradient Boosting* outperformed *Multi-Linear Regression* and *Deep Neural Networks* in both accuracy and interpretability. Its predictions provide valuable insights that hosts and property owners at Airbnb can use to optimize their listings and maximize revenue based on key features like *location, capacity, and amenities*"]},{"cell_type":"markdown","metadata":{"id":"3RXWpkaMyCzb"},"source":["## Table Cleanup\n","\n","This part is for cleaning up the Python tables that we will include in our report."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A69yAW3s9tky"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# add model column because plotly does not recognize index values\n","eval_metrics_df['Model'] = eval_metrics_df.index\n","eval_metrics_df = eval_metrics_df[['Model',\"MSE\", \"R²\", \"Adjusted R²\"]]\n","eval_metrics_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zazm4Jy1yQ6T"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# I used the example code from the offical website for pandas DataFrames as a template\n","# I also referenced the code under Alternating Row Colors (same source)\n","# Source: https://plotly.com/python/table/\n","\n","import plotly.graph_objects as go\n","\n","\n","# fig1 is the table comparing the MSE and R^2 scores (both adjusted and not adjusted) for all the models\n","headerColor = 'grey'\n","rowEvenColor = 'lightgrey'\n","rowOddColor = 'white'\n","\n","\n","fig1 = go.Figure(data=[go.Table(\n","    header=dict(values= eval_metrics_df.columns,\n","                fill_color='grey',\n","                align='left',\n","                font=dict(color='white'),\n","                line_color='black'),\n","    cells=dict(values= [eval_metrics_df[col] for col in eval_metrics_df.columns],\n","               fill_color = [[rowOddColor,rowEvenColor,rowOddColor, rowEvenColor,rowOddColor]*4],\n","               align='left',\n","               font=dict(color='black'),\n","               line_color='black',))\n","])\n","fig1.update_layout(title = \"Performance of Models\")\n","\n","fig1.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSrjSUclEKFp"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# making a dataframe for the gradient boosting model results\n","gb_test_results = pd.DataFrame(columns = ['Metric', 'Value'], data = [[\"Test MSE\",round(GB_mse,4)],[\"Test R²\", round(GB_r2,4)], [\"Test Adjusted R²\",round(GB_adjusted_r2,4)]])\n","gb_test_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4WZ53D5DmF4"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# fig2 is the results of the gradient boosting model\n","\n","fig2 = go.Figure(data=[go.Table(\n","    header=dict(values= gb_test_results.columns,\n","                fill_color='grey',\n","                align='left',\n","                font=dict(color='white'),\n","                line_color='black'),\n","    cells=dict(values= [gb_test_results[col] for col in gb_test_results.columns],\n","               fill_color = [[rowOddColor,rowEvenColor,rowOddColor, rowEvenColor,rowOddColor]*4],\n","               align='left',\n","               font=dict(color='black'),\n","               line_color='black',))\n","])\n","\n","fig2.update_layout(title = \"Performance of Gradient Boosting Model\")\n","\n","fig2.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvNQpJygG355"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# fig3 is the table of the Most Influential Features\n","\n","fig3 = go.Figure(data=[go.Table(\n","    header=dict(values= top_features_df.columns,\n","                fill_color='grey',\n","                align='left',\n","                font=dict(color='white'),\n","                line_color='black'),\n","    cells=dict(values= [top_features_df[col] for col in top_features_df.columns],\n","               fill_color = [[rowOddColor,rowEvenColor,rowOddColor, rowEvenColor,rowOddColor]*4],\n","               align='left',\n","               font=dict(color='black'),\n","               line_color='black',))\n","])\n","\n","fig3.update_layout(title = \"Most Influential Features\")\n","\n","fig3.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJOkG7xkG7Jt"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"]}],"source":["# fig4 is the table of the Most Influential Features\n","\n","fig4 = go.Figure(data=[go.Table(\n","    header=dict(values= last_features_df.columns,\n","                fill_color='grey',\n","                align='left',\n","                font=dict(color='white'),\n","                line_color='black'),\n","    cells=dict(values= [last_features_df[col] for col in last_features_df.columns],\n","               fill_color = [[rowOddColor,rowEvenColor,rowOddColor, rowEvenColor,rowOddColor]*4],\n","               align='left',\n","               font=dict(color='black'),\n","               line_color='black',))\n","])\n","\n","fig4.update_layout(title = \"Least Influential Features\")\n","\n","fig4.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0"}},"nbformat":4,"nbformat_minor":0}
